{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Video_Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWCLAk0tj36F",
        "colab_type": "text"
      },
      "source": [
        "#Getting Dataset and Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4gM_YLAoXrr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "55aa103c-63f5-4614-cf53-5a350e3eb52e"
      },
      "source": [
        "!git clone https://github.com/anubhavmaity/Sports-Type-Classifier"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Sports-Type-Classifier'...\n",
            "remote: Enumerating objects: 14521, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/14521)   \u001b[K\rremote: Counting objects:   1% (146/14521)   \u001b[K\rremote: Counting objects:   2% (291/14521)   \u001b[K\rremote: Counting objects:   3% (436/14521)   \u001b[K\rremote: Counting objects:   4% (581/14521)   \u001b[K\rremote: Counting objects:   5% (727/14521)   \u001b[K\rremote: Counting objects:   6% (872/14521)   \u001b[K\rremote: Counting objects:   7% (1017/14521)   \u001b[K\rremote: Counting objects:   8% (1162/14521)   \u001b[K\rremote: Counting objects:   9% (1307/14521)   \u001b[K\rremote: Counting objects:  10% (1453/14521)   \u001b[K\rremote: Counting objects:  11% (1598/14521)   \u001b[K\rremote: Counting objects:  12% (1743/14521)   \u001b[K\rremote: Counting objects:  13% (1888/14521)   \u001b[K\rremote: Counting objects:  14% (2033/14521)   \u001b[K\rremote: Counting objects:  15% (2179/14521)   \u001b[K\rremote: Counting objects:  16% (2324/14521)   \u001b[K\rremote: Counting objects:  17% (2469/14521)   \u001b[K\rremote: Counting objects:  18% (2614/14521)   \u001b[K\rremote: Counting objects:  19% (2759/14521)   \u001b[K\rremote: Counting objects:  20% (2905/14521)   \u001b[K\rremote: Counting objects:  21% (3050/14521)   \u001b[K\rremote: Counting objects:  22% (3195/14521)   \u001b[K\rremote: Counting objects:  23% (3340/14521)   \u001b[K\rremote: Counting objects:  24% (3486/14521)   \u001b[K\rremote: Counting objects:  25% (3631/14521)   \u001b[K\rremote: Counting objects:  26% (3776/14521)   \u001b[K\rremote: Counting objects:  27% (3921/14521)   \u001b[K\rremote: Counting objects:  28% (4066/14521)   \u001b[K\rremote: Counting objects:  29% (4212/14521)   \u001b[K\rremote: Counting objects:  30% (4357/14521)   \u001b[K\rremote: Counting objects:  31% (4502/14521)   \u001b[K\rremote: Counting objects:  32% (4647/14521)   \u001b[K\rremote: Counting objects:  33% (4792/14521)   \u001b[K\rremote: Counting objects:  34% (4938/14521)   \u001b[K\rremote: Counting objects:  35% (5083/14521)   \u001b[K\rremote: Counting objects:  36% (5228/14521)   \u001b[K\rremote: Counting objects:  37% (5373/14521)   \u001b[K\rremote: Counting objects:  38% (5518/14521)   \u001b[K\rremote: Counting objects:  39% (5664/14521)   \u001b[K\rremote: Counting objects:  40% (5809/14521)   \u001b[K\rremote: Counting objects:  41% (5954/14521)   \u001b[K\rremote: Counting objects:  42% (6099/14521)   \u001b[K\rremote: Counting objects:  43% (6245/14521)   \u001b[K\rremote: Counting objects:  44% (6390/14521)   \u001b[K\rremote: Counting objects:  45% (6535/14521)   \u001b[K\rremote: Counting objects:  46% (6680/14521)   \u001b[K\rremote: Counting objects:  47% (6825/14521)   \u001b[K\rremote: Counting objects:  48% (6971/14521)   \u001b[K\rremote: Counting objects:  49% (7116/14521)   \u001b[K\rremote: Counting objects:  50% (7261/14521)   \u001b[K\rremote: Counting objects:  51% (7406/14521)   \u001b[K\rremote: Counting objects:  52% (7551/14521)   \u001b[K\rremote: Counting objects:  53% (7697/14521)   \u001b[K\rremote: Counting objects:  54% (7842/14521)   \u001b[K\rremote: Counting objects:  55% (7987/14521)   \u001b[K\rremote: Counting objects:  56% (8132/14521)   \u001b[K\rremote: Counting objects:  57% (8277/14521)   \u001b[K\rremote: Counting objects:  58% (8423/14521)   \u001b[K\rremote: Counting objects:  59% (8568/14521)   \u001b[K\rremote: Counting objects:  60% (8713/14521)   \u001b[K\rremote: Counting objects:  61% (8858/14521)   \u001b[K\rremote: Counting objects:  62% (9004/14521)   \u001b[K\rremote: Counting objects:  63% (9149/14521)   \u001b[K\rremote: Counting objects:  64% (9294/14521)   \u001b[K\rremote: Counting objects:  65% (9439/14521)   \u001b[K\rremote: Counting objects:  66% (9584/14521)   \u001b[K\rremote: Counting objects:  67% (9730/14521)   \u001b[K\rremote: Counting objects:  68% (9875/14521)   \u001b[K\rremote: Counting objects:  69% (10020/14521)   \u001b[K\rremote: Counting objects:  70% (10165/14521)   \u001b[K\rremote: Counting objects:  71% (10310/14521)   \u001b[K\rremote: Counting objects:  72% (10456/14521)   \u001b[K\rremote: Counting objects:  73% (10601/14521)   \u001b[K\rremote: Counting objects:  74% (10746/14521)   \u001b[K\rremote: Counting objects:  75% (10891/14521)   \u001b[K\rremote: Counting objects:  76% (11036/14521)   \u001b[K\rremote: Counting objects:  77% (11182/14521)   \u001b[K\rremote: Counting objects:  78% (11327/14521)   \u001b[K\rremote: Counting objects:  79% (11472/14521)   \u001b[K\rremote: Counting objects:  80% (11617/14521)   \u001b[K\rremote: Counting objects:  81% (11763/14521)   \u001b[K\rremote: Counting objects:  82% (11908/14521)   \u001b[K\rremote: Counting objects:  83% (12053/14521)   \u001b[K\rremote: Counting objects:  84% (12198/14521)   \u001b[K\rremote: Counting objects:  85% (12343/14521)   \u001b[K\rremote: Counting objects:  86% (12489/14521)   \u001b[K\rremote: Counting objects:  87% (12634/14521)   \u001b[K\rremote: Counting objects:  88% (12779/14521)   \u001b[K\rremote: Counting objects:  89% (12924/14521)   \u001b[K\rremote: Counting objects:  90% (13069/14521)   \u001b[K\rremote: Counting objects:  91% (13215/14521)   \u001b[K\rremote: Counting objects:  92% (13360/14521)   \u001b[K\rremote: Counting objects:  93% (13505/14521)   \u001b[K\rremote: Counting objects:  94% (13650/14521)   \u001b[K\rremote: Counting objects:  95% (13795/14521)   \u001b[K\rremote: Counting objects:  96% (13941/14521)   \u001b[K\rremote: Counting objects:  97% (14086/14521)   \u001b[K\rremote: Counting objects:  98% (14231/14521)   \u001b[K\rremote: Counting objects:  99% (14376/14521)   \u001b[K\rremote: Counting objects: 100% (14521/14521)   \u001b[K\rremote: Counting objects: 100% (14521/14521), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14512/14512), done.\u001b[K\n",
            "remote: Total 14521 (delta 7), reused 14519 (delta 5), pack-reused 0\n",
            "Receiving objects: 100% (14521/14521), 592.88 MiB | 39.78 MiB/s, done.\n",
            "Resolving deltas: 100% (7/7), done.\n",
            "Checking out files: 100% (14619/14619), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj0ZBlSkrAax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "b95e0f43-fb6e-42a4-edea-f24c9b623e88"
      },
      "source": [
        "!pip install -U git+https://github.com/keras-team/keras git+https://github.com/keras-team/keras-applications"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/keras-team/keras\n",
            "  Cloning https://github.com/keras-team/keras to /tmp/pip-req-build-7vnspwwp\n",
            "  Running command git clone -q https://github.com/keras-team/keras /tmp/pip-req-build-7vnspwwp\n",
            "Collecting git+https://github.com/keras-team/keras-applications\n",
            "  Cloning https://github.com/keras-team/keras-applications to /tmp/pip-req-build-koxa5nsa\n",
            "  Running command git clone -q https://github.com/keras-team/keras-applications /tmp/pip-req-build-koxa5nsa\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.16.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: keras_applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: keras_preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.1.0)\n",
            "Building wheels for collected packages: Keras\n",
            "  Building wheel for Keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-drm21hzs/wheels/18/59/26/2a3c8d65212670e9526dcb6966eba15ee401e814aa74ca121d\n",
            "Successfully built Keras\n",
            "Installing collected packages: Keras\n",
            "  Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "Successfully installed Keras-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZfmamGSkAAR",
        "colab_type": "text"
      },
      "source": [
        "#Keras Modelling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFdVtPb2kEOr",
        "colab_type": "text"
      },
      "source": [
        "Importing keras modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezFpEjv_pQvM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d9d01e5-6c71-4986-9bc3-6ebd466bc5b1"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dense\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjVW9zZ4kIEm",
        "colab_type": "text"
      },
      "source": [
        "Image Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrBES2Nuq2CC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "b291a5ed-cf6e-4ba8-9d24-bc8d51ce21dc"
      },
      "source": [
        "Generator = keras.preprocessing.image.ImageDataGenerator(#rotation_range=25,\n",
        "                                                        #width_shift_range=0.2,\n",
        "                                                        #height_shift_range=0.2,\n",
        "                                                        #shear_range=0.2,\n",
        "                                                        #zoom_range=0.2,\n",
        "                                                        #fill_mode='nearest',\n",
        "                                                        horizontal_flip=True,\n",
        "                                                        #vertical_flip=False,\n",
        "                                                        validation_split = 0.1)\n",
        "shape = 224\n",
        "bs = 32\n",
        "classes = ['football','cricket','swimming']\n",
        "train_flow = Generator.flow_from_directory('Sports-Type-Classifier/data/', target_size=(shape, shape),\n",
        "                                           color_mode='rgb',\n",
        "                                           class_mode='categorical', batch_size=bs,\n",
        "                                           shuffle=True,\n",
        "                                           classes = classes,\n",
        "                                           subset = 'training')\n",
        "val_flow = Generator.flow_from_directory('Sports-Type-Classifier/data/', target_size=(shape, shape),\n",
        "                                           color_mode='rgb',\n",
        "                                           class_mode='categorical', batch_size=bs,\n",
        "                                           shuffle=True,\n",
        "                                           classes = classes,\n",
        "                                           subset = 'validation')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1920 images belonging to 3 classes.\n",
            "Found 212 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAy2QqPLkMuN",
        "colab_type": "text"
      },
      "source": [
        "Model using transfer learning on Resnet101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igjnn2qDrMSG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "36b7beba-2cdc-4f33-8b36-e69e7c143b62"
      },
      "source": [
        "base_model = keras.applications.resnet_v2.ResNet101V2(include_top=True, weights='imagenet', input_shape=(224,224,3), classes=1000)\n",
        "\n",
        "\"\"\"\n",
        "for i,layer in enumerate(base_model.layers):\n",
        "    print(i,layer.name)\n",
        "\"\"\"\n",
        "x = base_model.output\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "model = Model( inputs = base_model.input, outputs = predictions)\n",
        "#model.summary()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0717 07:08:52.067730 139866307049344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0717 07:08:52.107432 139866307049344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:529: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0717 07:08:52.117657 139866307049344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4420: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0717 07:08:52.146157 139866307049344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4255: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0717 07:08:52.167239 139866307049344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:178: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0717 07:08:52.168636 139866307049344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:185: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0717 07:08:54.797062 139866307049344 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2029: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet101v2_weights_tf_dim_ordering_tf_kernels.h5\n",
            "179519488/179518384 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnGa6z5Uk7-c",
        "colab_type": "text"
      },
      "source": [
        "Model Compilation and Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxJn5INRxdq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=Adam(lr = 0.01), metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kVGDvZMwCSS0",
        "colab": {}
      },
      "source": [
        "save = keras.callbacks.ModelCheckpoint('resnetv2_3_classes.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThzZymYPwb-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dd7edc8b-9c78-413e-c2ae-a93fd57b52d5"
      },
      "source": [
        "model.fit_generator(\n",
        "        train_flow,\n",
        "        steps_per_epoch=1920//32,\n",
        "        epochs=30,\n",
        "        validation_data=val_flow,\n",
        "        validation_steps=212//32,\n",
        "        callbacks = [save,lr])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0717 07:09:46.932595 139866307049344 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "60/60 [==============================] - 111s 2s/step - loss: 1.0053 - acc: 0.5339 - val_loss: 1.2750 - val_acc: 0.3177\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.27502, saving model to resnetv2_3_classes.h5\n",
            "Epoch 2/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.9243 - acc: 0.5547 - val_loss: 0.9131 - val_acc: 0.5444\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.27502 to 0.91308, saving model to resnetv2_3_classes.h5\n",
            "Epoch 3/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.7581 - acc: 0.6339 - val_loss: 0.6175 - val_acc: 0.7000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.91308 to 0.61749, saving model to resnetv2_3_classes.h5\n",
            "Epoch 4/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.6966 - acc: 0.6474 - val_loss: 0.7390 - val_acc: 0.6111\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.61749\n",
            "Epoch 5/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.6718 - acc: 0.6542 - val_loss: 0.5992 - val_acc: 0.6556\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.61749 to 0.59916, saving model to resnetv2_3_classes.h5\n",
            "Epoch 6/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.6304 - acc: 0.7036 - val_loss: 0.6703 - val_acc: 0.7167\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.59916\n",
            "Epoch 7/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.5855 - acc: 0.7417 - val_loss: 0.5964 - val_acc: 0.6722\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.59916 to 0.59641, saving model to resnetv2_3_classes.h5\n",
            "Epoch 8/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.5002 - acc: 0.7964 - val_loss: 2.0004 - val_acc: 0.3646\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.59641\n",
            "Epoch 9/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.4904 - acc: 0.8042 - val_loss: 0.5294 - val_acc: 0.8056\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.59641 to 0.52940, saving model to resnetv2_3_classes.h5\n",
            "Epoch 10/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.4626 - acc: 0.8141 - val_loss: 0.3682 - val_acc: 0.8611\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.52940 to 0.36825, saving model to resnetv2_3_classes.h5\n",
            "Epoch 11/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.4514 - acc: 0.8240 - val_loss: 0.6038 - val_acc: 0.7333\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.36825\n",
            "Epoch 12/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.4221 - acc: 0.8339 - val_loss: 0.2924 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.36825 to 0.29242, saving model to resnetv2_3_classes.h5\n",
            "Epoch 13/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.4444 - acc: 0.8208 - val_loss: 0.9397 - val_acc: 0.7111\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.29242\n",
            "Epoch 14/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.4190 - acc: 0.8307 - val_loss: 0.4709 - val_acc: 0.7722\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.29242\n",
            "\n",
            "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
            "Epoch 15/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3795 - acc: 0.8490 - val_loss: 0.2926 - val_acc: 0.9010\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.29242\n",
            "Epoch 16/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3591 - acc: 0.8646 - val_loss: 0.2753 - val_acc: 0.9056\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.29242 to 0.27531, saving model to resnetv2_3_classes.h5\n",
            "Epoch 17/30\n",
            "60/60 [==============================] - 84s 1s/step - loss: 0.3546 - acc: 0.8708 - val_loss: 0.3288 - val_acc: 0.8556\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.27531\n",
            "Epoch 18/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3408 - acc: 0.8729 - val_loss: 0.2941 - val_acc: 0.9000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.27531\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
            "Epoch 19/30\n",
            "60/60 [==============================] - 84s 1s/step - loss: 0.3393 - acc: 0.8760 - val_loss: 0.2742 - val_acc: 0.8778\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.27531 to 0.27419, saving model to resnetv2_3_classes.h5\n",
            "Epoch 20/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3342 - acc: 0.8740 - val_loss: 0.3264 - val_acc: 0.8889\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.27419\n",
            "Epoch 21/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3355 - acc: 0.8807 - val_loss: 0.2691 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.27419 to 0.26911, saving model to resnetv2_3_classes.h5\n",
            "Epoch 22/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3461 - acc: 0.8688 - val_loss: 0.3143 - val_acc: 0.8750\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.26911\n",
            "Epoch 23/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3337 - acc: 0.8776 - val_loss: 0.2865 - val_acc: 0.8944\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.26911\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "Epoch 24/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3370 - acc: 0.8729 - val_loss: 0.2580 - val_acc: 0.9000\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.26911 to 0.25800, saving model to resnetv2_3_classes.h5\n",
            "Epoch 25/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3367 - acc: 0.8760 - val_loss: 0.2811 - val_acc: 0.9278\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.25800\n",
            "Epoch 26/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3276 - acc: 0.8839 - val_loss: 0.3183 - val_acc: 0.8778\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.25800\n",
            "Epoch 27/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3330 - acc: 0.8776 - val_loss: 0.2781 - val_acc: 0.9056\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.25800\n",
            "Epoch 28/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3211 - acc: 0.8792 - val_loss: 0.2827 - val_acc: 0.9056\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.25800\n",
            "Epoch 29/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3216 - acc: 0.8812 - val_loss: 0.3026 - val_acc: 0.8802\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.25800\n",
            "Epoch 30/30\n",
            "60/60 [==============================] - 85s 1s/step - loss: 0.3302 - acc: 0.8818 - val_loss: 0.2768 - val_acc: 0.9167\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.25800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f348c961358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2jtJGfRVlQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights('resnetv2_3_classes.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prL0RrimlAsm",
        "colab_type": "text"
      },
      "source": [
        "#Predicting Video Category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UzlJdljPX5Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wh4MGGbrIJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(video_path,model,classes):\n",
        "    #################################\n",
        "    '''\n",
        "    INPUT-\n",
        "    \n",
        "    video_path -- path to the video to be predicted\n",
        "    \n",
        "    model -- the trained model to be used for prediction\n",
        "    \n",
        "    classes -- list of classes that model was trained on\n",
        "    \n",
        "    OUTPUT-\n",
        "    \n",
        "    All frames stored to data/ folder\n",
        "    \n",
        "    predicted class of every frame in a numpy array\n",
        "    '''\n",
        "    #################################\n",
        "    cam = cv2.VideoCapture(video_path)\n",
        "    os.makedirs('data')\n",
        "    # frame \n",
        "    currentframe = 0\n",
        "      \n",
        "    while 1:\n",
        "        #print(0)\n",
        "        # reading from frame \n",
        "        ret,frame = cam.read()   \n",
        "        if ret: \n",
        "            # if video is still left continue creating images \n",
        "            name = './data/frame' + str(currentframe) + '.jpg'\n",
        "            #print ('Creating...' + name)\n",
        "            # Cropping the extracted image\n",
        "            frame = cv2.resize(frame,(shape,shape))\n",
        "            # writing the extracted images \n",
        "            cv2.imwrite(name, frame)   \n",
        "            # increasing counter so that it will \n",
        "            # show how many frames are created \n",
        "            currentframe += 1\n",
        "        else: \n",
        "            break  \n",
        "    # Release all space and windows once done \n",
        "    cam.release() \n",
        "    cv2.destroyAllWindows()\n",
        "    \n",
        "    # Store all frame paths in a list\n",
        "    \n",
        "    #Data_generator for frames\n",
        "    pred_gen = keras.preprocessing.image.ImageDataGenerator()\n",
        "    pred_flow = pred_gen.flow_from_directory('', target_size=(shape, shape),color_mode='rgb',class_mode='categorical', batch_size=bs,shuffle=False, classes = ['data'])\n",
        "    \n",
        "    pred = model.predict_generator(pred_flow)\n",
        "    #the pred is probability of classes getting the max using argmax\n",
        "    pred = np.argmax(pred,axis = 1)\n",
        "    #getting the maximum occuring class across all frames\n",
        "    counts = np.bincount(pred)\n",
        "    print(classes[np.argmax(counts)])\n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lbR4Qgj7May",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -r data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5UZIIYo0e-B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "4f73a0d7-e06c-43c6-d0fc-3d37197f3513"
      },
      "source": [
        "pred_frame_wise = predict('30 Seconds of Premier League skills - HD.mp4',model = model,classes = classes)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 912 images belonging to 1 classes.\n",
            "football\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9ZkCpr81wlX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_vid_with_class(pred_frame_wise,classes):\n",
        "    #################################\n",
        "    '''\n",
        "    INPUT-\n",
        "    \n",
        "    pred_frame_wise -- numpy array of predicted class number of every frame\n",
        "        \n",
        "    classes -- list of classes that model was trained on\n",
        "    \n",
        "    OUTPUT-\n",
        "    \n",
        "    Video with class of every frame stored to disk as Output_video.avi\n",
        "    \n",
        "    '''\n",
        "    #################################    \n",
        "    output = cv2.imread('data/frame0.jpg')\n",
        "    height, width, layers = output.shape\n",
        "    size = (width,height)\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"DIVX\")\n",
        "    writer = cv2.VideoWriter(\"Output_video.avi\", fourcc, 30, size)\n",
        "    \n",
        "    for i in range(pred_frame_wise.shape[0]):\n",
        "        output = cv2.imread('data/frame' + str(i) + '.jpg')\n",
        "        # draw the activity on the output frame\n",
        "        text = \"activity: {}\".format(classes[pred_frame_wise[i]])\n",
        "        cv2.putText(output, text, (10, 20), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "        \t0.5, (25, 0, 190), 2)\n",
        "        \n",
        "        # write the output frame to disk\n",
        "        writer.write(output)\n",
        "    writer.release"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNQ7vPYVLVKT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_vid_with_class(pred_frame_wise,classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fqgndbLsBjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}